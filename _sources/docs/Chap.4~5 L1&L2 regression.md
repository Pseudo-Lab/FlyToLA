
## Index
- Linear regression
- ì„ í˜•ëŒ€ìˆ˜ ê´€ì ì—ì„œì˜ linear regression
- Least square estimation
- Ridge, Lasso regression
- numba


## Linear Regerssion
Linear regression : ì¢…ì†ë³€ìˆ˜ yì™€ í•œ ê°œ ì´ìƒì˜  ë³€ìˆ˜ x ì™€ì˜ ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë¶„ì„ ê¸°ë²•

Regression : â€˜**íšŒê·€**â€™ ì˜ ì‚¬ì „ì  ì˜ë¯¸ :  â€˜í•œ ë°”í€´ ëŒì•„ ì œìë¦¬ë¡œ ëŒì•„ê°€ë‹¤' .. ê·¸ë ‡ê°€ë©´ ë­ê°€ ì–´ë””ë¡œ ëŒì•„ê°€ëŠ” ê²ƒì¸ê°€? 

ë°ì´í„°ê°€ ì¶”ì„¸ì„ ìœ¼ë¡œ íšŒê·€í•œë‹¤?  function approximation?   
ì˜¤ì°¨ì˜ í•©ì´ ìµœì†Œí™”ë˜ë„ë¡ í•˜ëŠ” ì„ ì„ ì°¾ëŠ” ê²ƒì€ ì§ê´€ì ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê¸°ì— ê·¸ëŸ´ë“¯ í•˜ë‹¤.  ê·¸ëŸ°ë° â€˜íšŒê·€â€™ë¼ëŠ” ë§ê³¼ëŠ” ë¬´ìŠ¨ ê´€ê³„ì¼ê¹Œ?

![img](img/ch4/1.png)

Leas squares method(OLS)ëŠ” gauss markov theorem í•˜ì—ì„œ best linear unbiased estimator ì´ë‹¤.  
ì”ì°¨ê°€ iid (independent and identically distrubuted random) ê°€ì •ì„ ë§Œì¡±í•œë‹¤ë©´,
- ì”ì°¨ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì´ë‹¤
- ì”ì°¨ì™€ ë…ë¦½ë³€ìˆ˜ X ì‚¬ì´ì— ìƒê´€ê´€ê³„ê°€ ì—†ê³ , ìê¸° ìì‹ ê³¼ë„ ìƒê´€ê´€ê³„ê°€ ì—†ë‹¤.(independent)
- ì”ì°¨ì˜ ë¶„í¬ê°€ ì¼ì •í•´ì•¼í•œë‹¤ (ë“±ë¶„ì‚°ì„± ë§Œì¡±)

ì¦‰ íšŒê·€ëª¨ë¸ì´ë€ ì”ì°¨(residual)ê°€ í‰ê· ìœ¼ë¡œ íšŒê·€í•˜ê²Œ ë§Œë“  ëª¨ë¸ì´ë‹¤.



## ì„ í˜•ëŒ€ìˆ˜í•™ì˜ ê´€ì ì—ì„œ ë³¸ íšŒê·€ë¶„ì„
(-1,0) , (0,1) , (0,3) ì˜ ì„¸ ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ì£¼ì–´ì ¸ ìˆë‹¤ê³  í•˜ì.  
ë§Œì¼ $f(x) = mx + b $ ì§ì„ ì´ ì„¸ ì ì„ í†µê³¼í•œë‹¤ë©´ ì—°ë¦½ë°©ì •ì‹ì€ ë‹¤ìŒ í–‰ë ¬ì‹ì„ í‘¸ëŠ” ê²ƒê³¼ ê°™ë‹¤.

![img](img/ch4/2.png)
$
(Ax = b )  â‡’  \begin{bmatrix}
  -1 & 1  \\
   0 & 1  \\
   0 & 1
\end{bmatrix}
 \begin{bmatrix}
  m \\
  b
\end{bmatrix}
=  \begin{bmatrix}
  0 \\
  1 \\
  3
\end{bmatrix}
$

$ 
A\overrightarrow{x} = \overrightarrow{b} 
$

$
  â‡’  \begin{bmatrix}
  | & |  \\
  \overrightarrow{a_1} & \overrightarrow{a_2}  \\
  | & |
\end{bmatrix}
 \begin{bmatrix}
  x_1 \\
  x_2
\end{bmatrix}
=  \begin{bmatrix}
  | \\
  b \\
  |
\end{bmatrix}
$ 
$
  â‡’   
  x_1 
\begin{bmatrix}
  | \\
  \overrightarrow{a_1}\\
  | 
\end{bmatrix}
  x_2 
\begin{bmatrix}
  | \\
  \overrightarrow{a_2}\\
  | 
\end{bmatrix}
=  \begin{bmatrix}
  | \\
  b \\
  |
\end{bmatrix}
$

![img](img/ch4/3.png)

í–‰ë ¬ Aì˜ ì—´ë²¡í„° $\overrightarrow{a_1}$ , $\overrightarrow{a_2}$ ì˜ ì„ í˜•ê²°í•© ì„ í†µí•´ $\overrightarrow{b}$  ë¥¼ êµ¬í•˜ì.  
$\overrightarrow{b}$  ê°€ $\overrightarrow{a_1}$ , $\overrightarrow{a_2}$ ì˜ span ì•ˆì— ìˆë‹¤ë©´ êµ¬í•  ìˆ˜ ìˆë‹¤.

![img](img/ch4/4.png)


í•˜ì§€ë§Œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ $\overrightarrow{b}$ ê°€ $col(A)$ ì˜ span ë°–ì— ìˆë‹¤ë©´?    
$\overrightarrow{b}$ ë¥¼ $col(A)$ ì— orthogonal projection ì‹œí‚¨ $\overrightarrow{p}$ ê°€ span ë‚´ì˜ ìµœì ì˜ ë²¡í„°ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.  
$\overrightarrow{e} =\overrightarrow{b} -\overrightarrow{p} $ ì´ë©°  í–‰ë ¬ Aì˜ ì–´ë–¤ ë²¡í„°ì™€ë„ ì§êµí•˜ë¯€ë¡œ ë‹¤ìŒ ìˆ˜ì‹ì´ ì„±ë¦½í•œë‹¤.
$$
A\cdot\overrightarrow{e} = \begin{bmatrix}
  | & |  \\
  \overrightarrow{a_1} & \overrightarrow{a_2}  \\
  | & |
\end{bmatrix}\cdot\overrightarrow{e} = 0 $$

$A^{T}(\overrightarrow{b} - A\hat{x})=0\\
A^{T}\overrightarrow{b} - A^{T}A\hat{x}=0 \\
A^{T}A\hat{x}= A^{T}\overrightarrow{b}\\
\hat{x} = (A^{T}A)^{-1}A^{T}\overrightarrow{b}$ 

https://angeloyeo.github.io/2020/08/24/linear_regression.html


## Least square estimation method 

í‰ê· ì œê³±ì˜¤ì°¨ (MSE) ë¥¼ ìµœì†Œí™”í•˜ëŠ” íšŒê·€ê³„ìˆ˜ $B = (B_1,...B_p)$ ë¥¼ ê³„ì‚°í•œë‹¤.
beta ì— ëŒ€í•œ unbiased estimatorì¤‘ ê°€ì¥ ë¶„ì‚°ì´ ì‘ì€ estimatorë¥¼ BLUEë¼ê³  í•œë‹¤.

$$\hat{\beta}^{LS} = argmin_\beta \{\sum^{n}_{i=1}(y_i -x_i\beta)^2\} = (X^{T}X)^{-1}X^{T}y$$

![img](img/ch4/14.png)

Irreducible Error ì€ ëª¨ë¸ì´ í•´ê²°ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ì´ë‹¤.  
ê·¸ë ‡ë‹¤ë©´ Bias ì™€ Variance ì¤‘ ë¬´ì—‡ì„ ì¤„ì´ëŠ”ë° ì§‘ì¤‘í•  ê²ƒì¸ê°€? 
* ë² íƒ€ì˜ ê°œìˆ˜ p ë³´ë‹¤ ë°ì´í„°ì˜ ìˆ˜ nì´ â€˜ì¶©ë¶„íˆ' ë§ìœ¼ë©´ì„œ ê°€ì •ëœ ë¶„í¬ê°€ ì–´ëŠì •ë„ ë§ë‹¤ë©´ least squares ëŠ” ìƒë‹¹íˆ ì¢‹ì€ low bias ì˜ ì¶”ì •ì„ í•œë‹¤

* ë°˜ëŒ€ë¡œ ë² íƒ€ì˜ ê°œìˆ˜ pê°€ n ë³´ë‹¤ ì¶©ë¶„íˆ ë§ì§€ ì•Šìœ¼ë©´ overfitting, ì ë‹¤ë©´ ols ì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ (X(n*p)ê°€ linearly dependent). ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì‚´í´ë³´ì

  1) subset selection : ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ ì„ íƒ

  2) shrinkage : regularization (betaì˜ ê°’ì„ ì¶•ì†Œì‹œí‚¨ë‹¤)

  3) dimension reduction


## Regularization  

ì¢‹ì€ ëª¨ë¸ì´ë€ ë¬´ì—‡ì¼ê¹Œ?  
í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ë ¥ì´ ë†’ìœ¼ë©´ì„œ, ë¯¸ë˜ì— ëŒ€í•œ ì˜ˆì¸¡ëŠ¥ë ¥ë„ ì¢‹ì€ ëª¨ë¸ì¼ ê²ƒì´ë‹¤.  
ì¦‰, ì´ìƒì ì¸ ëª¨ë¸ì€ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì—ì„œÂ ë°˜ë³µë˜ëŠ” ê·œì¹™ì„±ì„ ì •í™•í•˜ê²Œ ì¡ì•„ë‚´ë©´ì„œë„ í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì˜ ì¼ë°˜í™” í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

![img](img/ch4/5.png)
![img](img/ch4/6.png)

- 1ë²ˆ ì„ í˜• ëª¨ë¸ : underfit , high bias, low variance
- 3ë²ˆ ê³ ì°¨ ë‹¤í•­í•¨ìˆ˜ ëª¨ë¸ : **overfit , high bias , high variance**
- 2ë²ˆì´ ì´ ì¤‘ì—ëŠ” ì ì ˆí•˜ë‹¤


## Regularization concept

í˜„ì¬ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ ë§ê³ ë„ ë¯¸ë˜ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ ë†’ì¼ ìˆ˜ ìˆì„ê¹Œ.   
ë² íƒ€ì— ëŒ€í•œ ì œì•½(penalty)ì„ ì¤˜ì„œ generalization accuracy ë„ ê³ ë ¤ë¥¼ í•´ë³´ì


$ L(\beta) = \underset{\beta}{min}\underset{i=1}{\sum} (y_i -\hat{y_i})^2 + \lambda\overset{p}{\underset{j=1}{\sum}}\beta_j^2 $

$\lambda$  : `regularization parameter` that controls trade-off between bias and variance 

1) $\lambda$  very big : ë§Œì•½ 10,000ì´ë¼ë©´ ë² íƒ€ëŠ” 0ì´ ëœë‹¤.. - underfitting

2) $\lambda$  very small : ë§Œì•½ 0ì´ë©´ least squaresì™€ ë™ì¼í•˜ë‹¤. high variance -overfitting


- regularization methodëŠ” íšŒê·€ ê³„ìˆ˜ betaê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì— ì œì•½ì¡°ê±´ì„ ë¶€ì—¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- ì œì•½ì¡°ê±´ì— ì˜í•´ biasê°€ ì¦ê°€í•  ìˆ˜ ìˆì§€ë§Œ varianceëŠ” ê°ì†Œí•œë‹¤.


![img](img/ch4/7.png)


## Ridge regression 

- **L2-norm regularization**
- ì”ì°¨ì˜ ì œê³±ì„ ìµœì†Œí™”í•˜ë©´ì„œë„ íšŒê·€ ê³„ìˆ˜ ë² íƒ€ì˜ L2-norm ì„ ì œí•œí•œë‹¤
- ì•„ë˜ ë‘ ì‹ì€ ë™ì¼í•˜ë‹¤
$$ 1. \ \hat{\beta}^{ridge} = \underset{\beta}{argmin}\{\overset{n}{\underset{i=1}{\sum}} (y_i -x_i\beta)^2 +
 \lambda\overset{p}{\underset{j=1}{\sum}}{\beta}_j^2\} $$

$$ 2. \ \hat{\beta}^{ridge} = \underset{\beta}{argmin}\overset{n}{\underset{i=1}{\sum}} (y_i -x_i\beta)^2 \\
subject\ to\ \overset{p}{\underset{j=1}{\sum}}{\beta}_j^2\leq{t} $$

- MSE Contour

![img](img/ch4/8.png)

Conic equation ì˜ Discriminant (íŒë³„ì‹) < 0 ì¼ ë•Œ,  ellipse (íƒ€ì›) ì˜ í˜•íƒœë¥¼ ëˆë‹¤.

- $B^2 -4AC < 0 $   : ellipse(íƒ€ì›)
- $B^2 -4AC > 0$  :  hyperbola (ìŒê³¡ì„ )
- $B^2 -4AC = 0$  : parbola (í¬ë¬¼ì„ )
- $B=0, \ A = C$ : circle(ì›)

![img](img/ch4/9.png)

ridgeì˜ ì œì•½ë²”ìœ„ëŠ” ì›ì´ê¸° ë•Œë¬¸ì— ì¶•ì—ì„œ êµì ì´ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ë² íƒ€ê°€ 0ì´ë˜ì§€ëŠ” ì•Šìœ¼ë©´ì„œ 0ì˜ ë°©í–¥ìœ¼ë¡œ shrinkí•œë‹¤.  
ë°˜ë©´ lassoì˜ ì œì•½ë²”ìœ„ëŠ” ì‚¬ê°í˜•ì´ë¯€ë¡œ ì¶•ì—ì„œ êµì ì´ ìƒê¸´ë‹¤. ê·¸ë¦¼ì—ì„œëŠ” $\beta_2$=0 ìœ¼ë¡œ  $\beta_2$ëŠ” ì œì™¸ë˜ê²Œ ëœë‹¤. (MSEê°€ ì»¤ì§€ë”ë¼ë„, ë”°ë¼ì„œ biasê°€ ì»¤ì§€ë”ë¼ë„ varianceë¥¼ ì¤„ì´ëŠ” ë°©ì‹)  
ì°¨ì›ì´ ì»¤ì§€ë”ë¼ë„ (d=3)  ì œì•½ë²”ìœ„ëŠ” ridge ëŠ” êµ¬, lassoëŠ” ë‹¤ë©´ì²´ê°€ ë˜ë¯€ë¡œ ê°™ì€ ì„±ì§ˆì„ ìœ ì§€í•œë‹¤.
![img](img/ch4/10.png)

- Ridge ëŠ” í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ closed form solution ì„ êµ¬í•  ìˆ˜ ìˆë‹¤

![img](img/ch4/11.png)


### Lasso regression

- Least Absolute Shrinkage and Selection Operator (lasso)
- L1-norm regularization : íšŒê·€ ê³„ìˆ˜ ë² íƒ€ì˜ L1-norm ì„ ì œí•œ
- sparse solution

$
\ \hat{\beta}^{lasso} = \underset{\beta}{argmin}\overset{n}{\underset{i=1}{\sum}} (y_i -x_i\beta)^2 \\
subject\ to\ \overset{p}{\underset{j=1}{\sum}}{|\beta|}_j\leq{t} 
$

![img](img/ch4/12.png)

- Ridge ì™€ ë‹¬ë¦¬ Lasso fomulation ì€ closed form solution ì„ êµ¬í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤

- Numerical optimization method
  - Quadratic programming techiniques
  - LARS algorithm
  - Coordinate descent algorithm  


- Tuning parameter($\lambda$)

  Ridgeì™€ LassoëŠ”Â Î»ë¥¼ ì¡°ì •í•˜ì—¬ ìµœì ëª¨í˜•ì„ êµ¬í•˜ë©° ê·¸ ê³¼ì •ì—ì„œ Cross-Validationì„ ì‚¬ìš©í•œë‹¤

  [1] ê³ ë ¤í• Â Î»ì˜ ë²”ìœ„(grid)ë¥¼ ì„ íƒí•œë‹¤.  
  [2] ê°Â Î»ê°’ì— ëŒ€í•´ cross-validationÂ errorë¥¼ êµ¬í•œë‹¤.  
  [3] errorë¥¼ ê°€ì¥ ì‘ê²Œ í•´ì£¼ëŠ”Â Î»ê°’ì„ ì„ íƒí•œë‹¤.  
  [4] ë§ˆì§€ë§‰ìœ¼ë¡œ, ì„ íƒëœÂ Î»ê°’ì„ ì´ìš©í•´ ëª¨í˜•ì„ ì¬ì í•©ì‹œí‚¨ë‹¤.



  ## Conclusion 

- Ridge, Lasso ëŠ” ëª¨ë‘ ë¶„ì‚°(variance)ì„ ì¤„ì´ê¸° ìœ„í•´ íšŒê·€ê³„ìˆ˜ $\beta$ ë¥¼ shrink í•˜ëŠ” methodì´ë‹¤
- ì¼ë°˜ì ìœ¼ë¡œ ridgeëŠ” ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ê²½ìš°ì—(collinearlity) ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì´ë‚˜
ëª¨ë“  ê³„ìˆ˜ë“¤ì„ ì™„ì „íˆ 0ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ì§€ ì•Šê¸° ë•Œë¬¸ì—, ëª¨ë¸ì´ ë³µì¡í•´ì§€ê³  í•´ì„ì´ ì–´ë µë‹¤
- Lasso ëŠ” íšŒê·€ê³„ìˆ˜ ì¶”ì •ì¹˜ë¥¼ ì™„ì „íˆ 0ìœ¼ë¡œ ìˆ˜ì¶•ì‹œí‚¤ê¸° ë•Œë¬¸ì— ë³€ìˆ˜ ì„ íƒì´ ê°€ëŠ¥í•˜ê³  ì„¤ëª…ë ¥ì´ ì¢‹ë‹¤.
ìƒëŒ€ì ìœ¼ë¡œ ì˜ˆì¸¡ë³€ìˆ˜ë“¤ ì¤‘ ì¼ë¶€ë¶„ì˜ ì˜ˆì¸¡ë³€ìˆ˜ê°€ í° íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ê³  ë‚˜ë¨¸ì§€ ì˜ˆì¸¡ë³€ìˆ˜ê°€ ì‘ì€ íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ê±°ë‚˜ ê±°ì˜ 0ê³¼ ë™ì¼í•œ ìƒí™©ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ê°€ì§„ë‹¤.


## Numba


<aside>
ğŸ’¡   
íŒŒì´ì¬ì€ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ë¡œì„œ C/C++/Fortran ê³¼ ê°™ì€ ì»´íŒŒì¼ ì–¸ì–´ì— ë¹„í•´ ì†ë„ê°€ ëŠë¦¬ë‹¤.

Numbaë¡œ ë°ì½”ë ˆì´íŠ¸ëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ í•´ë‹¹ í•¨ìˆ˜ê°€ ì¦‰ì„ì—ì„œ ë°”ë¡œ ê¸°ê³„ì–´ ì½”ë“œë¡œ ì»´íŒŒì¼ë˜ê³  ì›ë˜ì˜ ê¸°ê³„ì–´ ì½”ë“œ ì†ë„ë¡œ ì‹¤í–‰ëœë‹¤.
ì»´íŒŒì¼ì´ í•œë²ˆ ì™„ë£Œë˜ë©´ NumbaëŠ” í•´ë‹¹ ê¸°ê³„ì–´ ì½”ë“œ ë²„ì „ì„ ì €ì¥í•´ ë†“ëŠ”ë‹¤. ê·¸ë¦¬ê³  ê°™ì€ íƒ€ì…ìœ¼ë¡œ ë‹¤ì‹œ í˜¸ì¶œëœë‹¤ë©´ ë‹¤ì‹œ ì»´íŒŒì¼í•˜ì§€ ì•Šê³  ë¯¸ë¦¬ ì €ì¥ëœ ë²„ì „ì„ ì¬ì‚¬ìš©í•œë‹¤.

</aside>

- ì „ì²´ ì• í”Œë¦¬ì¼€ì´ì…˜, í”„ë¡œê·¸ë¨ì´ ì•„ë‹Œ ë°ì½”ë ˆì´í„°ë¡œ ì¥ì‹ëœ í•¨ìˆ˜ì— ëŒ€í•´ì„œë§Œ ë³„ë„ë¡œ ì»´íŒŒì¼í•œë‹¤.
- íŠ¹ì • ìˆ˜ì¹˜í˜• íƒ€ì…(int,float,complex)ì— ëŒ€í•´ì„œ ë¹ ë¥¸ ì—°ì‚°ì„ ì œê³µí•˜ë¯€ë¡œ numpy arrayì™€ ì¼ë°˜ì ìœ¼ë¡œ ê°™ì´ ì‚¬ìš©í•œë‹¤
- í•¨ìˆ˜ê°€ ì²˜ìŒ í˜¸ì¶œë˜ì—ˆì„ ë•Œ ì»´íŒŒì¼í•œë‹¤(JIT)


### Jit

Jitì€ just in timeì˜ ì•½ìë¡œ, ë°ì½”ë ˆì´í„°ê°€ í˜¸ì¶œë  ë•Œ í•¨ìˆ˜ê°€ ì»´íŒŒì¼ë˜ê³  ê·¸ ì´í›„ì—ëŠ” ì»´íŒŒì¼ëœ ë¶€ë¶„ì´ ìœ ì§€ë˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ ë£¨í”„ë³´ë‹¤ ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤.  
ë˜í•œ ì»´íŒŒì¼ ì˜ì—­ì— ì„ ì–¸ë˜ì–´ìˆì–´ better localityë¥¼ ê°€ì§„ë‹¤.

```python
@jit()
def proc_numba(xx,yy,zz):
    for j in range(nobs):   
        x, y = xx[j], yy[j] 
        x = x*2 - ( y * 55 )
        y = x + y*2         
        z = x + y + 99      
        z = z * ( z - .88 ) 
        zz[j] = z           
    return zz
```

### Vectorize
- Numbaâ€™s vectorize allows Python functions taking scalar input arguments to be used as NumPyÂ [ufuncs](http://docs.scipy.org/doc/numpy/reference/ufuncs.html)
- ufuncs : universial function, array broadcasting, type casting ì„ ì§€ì›í•˜ëŠ” operation function
- vectorizeÂ ë°ì½”ë ˆì´í„°ì— ì‹œê·¸ë„ˆì³ë¥¼ ë„˜ê¸°ë©´, íŒŒì´ì¬ í•¨ìˆ˜ëŠ” Numpy ufuncìœ¼ë¡œ ì»´íŒŒì¼í•´ì¤€ë‹¤

```python
from numba import vectorize, float64

@vectorize([float64(float64, float64)])
def f(x, y):
    return x + y

a = range(6)
f(a,a)
>>> array([ 0.,  2.,  4.,  6.,  8., 10.])
```

ì™œ @jit ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ë‹¨ìˆœ ë°˜ë³µ ë£¨í”„ë¥¼ ì»´íŒŒì¼í•˜ì§€ ì•Šê³  vectorizeë¥¼ ì“¸ê¹Œ
Numpy ufunc ì˜ íŠ¹ì§•ë“¤ (broadcasting, reduction, accumulation ë“±) ì„ ì´ìš©í•˜ê¸° ìœ„í•´ì„œ

```python
a= np.arange(12).reshape(3,4)/1.5
a
>>> array([[0.        , 0.66666667, 1.33333333, 2.        ],
       [2.66666667, 3.33333333, 4.        , 4.66666667],
       [5.33333333, 6.        , 6.66666667, 7.33333333]])

f.reduce(a,axis=0)
>>> array([ 8., 10., 12., 14.])

f.accumulate(a) 
>>> array([[ 0.        ,  0.66666667,  1.33333333,  2.        ],
       [ 2.66666667,  4.        ,  5.33333333,  6.66666667],
       [ 8.        , 10.        , 12.        , 14.        ]])
```

- numbaëŠ” ë§ì€ ìˆ˜ì˜ ë°˜ë³µë¬¸ì„ ì‹¤í–‰í•´ì•¼ ë˜ëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ê²½ìš° ê°„
ë‹¨íˆ ë°ì½”ë ˆì´í„°ë§Œì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì¼ë°˜ì ì¸ íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤
- pandasì™€ ê°™ì´ ë°ì´í„°í”„ë ˆì„ì„ ë‹¤ë£¨ëŠ” ê³ ìˆ˜ì¤€ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ëŠ” í˜¸í™˜ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤

### other decorators

- `@njit`Â - ì´ê²ƒì€Â `@jit(nopython=True)`ì— ëŒ€í•œ ë³„ì¹­ì´ë©°, ë§¤ìš° ìì£¼ ì“°ì´ëŠ” ë°ì½”ë ˆì´í„°ì´ë‹¤.
- `@vectorize`Â - NumPyÂ `ufunc`ì„ ìƒì„±í•œë‹¤. (ëª¨ë“ Â `ufunc`Â ë©”ì˜ë“œê°€ ì§€ì›ëœë‹¤)
- `@guvectorize`Â - NumPyÂ `generalized ufunc`ì„ ìƒì„±í•œë‹¤.Â 
- `@stencil`Â - ì—°ì‚°ê³¼ ê°™ì€ ìŠ¤í…ì‹¤ì„ ìœ„í•œ ì»¤ë„ë¡œì„œì˜ í•¨ìˆ˜ë¥¼ ì„ ì–¸í•œë‹¤.Â 
- `@jitclass`Â - jitì„ ì•Œê³  ìˆëŠ” í´ë˜ìŠ¤ ìƒì„±ìš©.Â 
- `@cfunc`Â - C/C++ë¡œë¶€í„° í˜¸ì¶œë  ìˆ˜ ìˆëŠ” ë„¤ì´í‹°ë¸Œ ì½œë°±ìœ¼ë¡œ ì‚¬ìš©ë  í•¨ìˆ˜ë¥¼ ì„ ì–¸í•œë‹¤.
- `@overload`Â -Â `@overload(scipy.special.j0)`ì™€ ê°™ì€ ì˜ˆì²˜ëŸ¼, ê¸°ì¡´ì˜ ì–´ë–¤ í•¨ìˆ˜ë¥¼ nopython ëª¨ë“œë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ í•´ë‹¹ í•¨ìˆ˜ì— ëŒ€í•´ì„œ ë³„ë„ì˜ êµ¬í˜„ë¬¼ì„ ë“±ë¡í•œë‹¤.Â 


## Reference
https://brunch.co.kr/@gimmesilver/17
https://angeloyeo.github.io/2020/08/24/linear_regression.html